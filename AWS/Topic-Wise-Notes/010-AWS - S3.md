AWS - S3 : SimpleStorageService
-----------------------------------------------------------------------------------------------------------------------------------------------------------

AWS S3: 
    Its advertised as the infinitely scaling storage, hence we do not define its size while provisioning it (unlink EBS or EFS). It allows the users to 
    store objects (files) in 'buckets' (directories).
    
    a.  Buckets MUST have a globally-unique-name
    b.  Buckets are defined at the regional level
    c.  Follows a specific naming convention
        1.  No Uppercase
        2.  NO underscore
        3.  Between 3-63 characters long
        4.  Not an IP
        5.  Must Start with lowercase letter or number

    d.  OBJECTS (files) have a key, the key is basically the complete path to the file, for.e.g.
        1.  s3://my-bucket/my_file.txt                              :   here the 'key' is : my_file.txt
        2.  s3://my-bucket/root-folder/child-folder/my_file.txt     :   here the 'key' is : root-folder/child-folder/my_file.txt

    e.  OBJECT (files) have a value as well. The values of these keys is basically, the content of the file
        1.  Maximum Object size in S3 is 5TB
        2.  If uploading more than 5GB in one go, the user must use 'Multi-Part' upload.
        3.  If uploading a file more than 160 GB, you must use AWS-CLI, AWS SDK, or AWS S3 Rest API
        4.  Each object in S3, can have 'metadata' (filename, fileowner, created-date, some system data etc)
        5.  Each object in S3, can have Tags (a maximum of 10, which is useful for security / identification and lifecycle management)
        6.  Each object in S3, can have its Version ID (if versioning is enabled)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q01: What is the minimum policy required to be able to create a S3 Bucket?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
The minimum role required for a user to be able to create an S3 bucket is - AmazonS3FullAccess

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q02: How to create a S3 Bucket?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    Following are the steps to create a S3 bucket
        a.  Login to the AWS management console with a user that has the permissions to create a S3 Bucket
        b.  Search for S3 on the AWS management console
        c.  On the S3 dashboard, click on 'Create Bucket'
        d.  Provide a 'Bucket Name' to the bucket [This name must be globally unique]
        e.  Select the AWS region where you want to create the bucket 
            [Even though S3 is global service, but buckets are regional]
        f.  On the 'Object Ownership section, select 'ACLs Disabled' 
            [All accounts in this bucket are owned by this account, Access to the bucket 
            and its objects is specified using only policies]
        g.  'Block all public access' is enabled by default.
        h.  On 'Bucket Versioning', 'disabled' is selected by default
        i.  Default Encryption is selected as - 'Amazon-S3-managed-keys'
        j.  Under 'Advanced Settings' - Object Lock is 'disabled' by default
            [Store objects using a write-once-read-many (WORM) model to help you prevent objects from being 
            deleted or overwritten for a fixed amount of time or indefinitely]
        k.  Once the bucket is created, you can click on the bucket name and check the properties / permissions 
            / Metrics / Access points etc.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q03: How to upload an object in to an S3 bucket?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    Once the S3 bucket is created:
        a.  Click on the 'Buckets' section of S3 Dashboard.
        b.  Click on the bucket name, and then click on upload.
        c.  Under 'Upload' click on 'Add Files' or 'Add Folders' and navigate to the file or folder on your machine.
        d.  'Destination URL' will be prebuilt based on your S3 bucket.
        e.  Select the 'Storage Class' and then click on 'Upload'
        f.  Once the object is uploaded successfully, it will visible under the 'Bucket' --> 'Objects'

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q04: How many storage classes are there in S3?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    S3 has the following storage classes.
        a.  Standard                :   Designed for                    : Frequently Accessed Data (more than once a month)
                                                                            with milliseconds access
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : None
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : None
        
        b.  Intelligent Tiering     :   Designed for                    : Data with changing and unknown access patterns
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : None
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : Per-object fees apply for objects >= 128kb
                                        Retrieval fees                  : None

        c.  Standard-IA             :   Designed for                    : Infrequently accessed data (once a month) with milliseconds access
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 30 Days
                                        Minimum Billable Object size    : 128 KB
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        d.  One Zone-IA             :   Designed for                    : Re-creatable, infrequently accessed data (once a month) stored in a single 
                                                                          Availability Zone with milliseconds access
                                        Available in                    : 1 AZ
                                        Minimum Storage Duration        : 30 Days
                                        Minimum Billable Object size    : 128 KB
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        e.  Glacier Instant Retrieval:  Designed for                    : Long-lived archive data accessed once a quarter with instant retrieval in 
                                                                          milliseconds
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 90 Days
                                        Minimum Billable Object size    : 128 KB
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        f.  Glacier Flexible Retrieval: Designed for                    : Long-lived archive data accessed once a year with retrieval of minutes to hours
            (formerly Glacier)
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 90 Days
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        g.  Glacier Deep Archive    :   Designed for                    : Long-lived archive data accessed less than once a year with retrieval of hours
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 180 Days
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply

        h.  Reduced Redundancy      :   Designed for                    : Noncritical, frequently accessed data with milliseconds access 
                                                                          (not recommended as S3 Standard is more cost effective)
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : None
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q05: How to check the properties of an uploaded object?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    Following are the steps:
    a.  Once you are inside the S3 bucket, click on the object, you wish to check the properties for.
    b.  A new standard dialog will open and the 'Properties' tab will be pre-selected, their are two more tabs, Permissions and Versions
    c.  On this page, their are 4 more options on the right top [Copy S3 URI, DOWNLOAD, OPEN, Object Options]
    d.  If you wish to see the object content without downloading it, simply click on 'OPEN'
    e.  When you click on 'Open', AWS opens a link which points to this object and has a pre-signed URL with AWS security token
    f.  If you try to open the same object using 'Object URL', it will fail, since this URL does not have the pre-signed AWS security 
        token

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q06: Explain key points of S3 Bucket versioning feature?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    Amazon S3 allows for versioning of the files
        a.  Its enabled at the bucket level
        b.  Same Key overwrite will increment the 'Version': 1,2,3 etc.
        c.  Its a best practice to version your buckets.
            1.  So that you are protected against unintended deletes (ability to restore a version)
            2.  Easy Rollback to a previous version
        
        Special Note:
            1.  Any file, that is not versioned prior to enabling versioning, will have version id as NULL
            2.  Suspending versioning, does not delete the previous versions.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q07: What happens if you upload a file with the same name with S3 Bucket versioning enabled?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
If we upload two files of the same name with S3 bucket versioning enabled, then both the file will be enabled and will have 'Version ID' populated with some 'string value' as the version id of both the files.The version of the file can be seen, under the 'Bucket' - 'Show Versions', and both the files with the same name but different version ids will be shown to the user.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q08: What is the difference between 'delete' vs 'Permanently Delete' in S3 Bucket?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Suppose you attempt to delete a file, (which has multiple versions, but since 'Show Versions' is disabled, you cannot see those versions). using the 'checkbox' and 'delete' button on the s3 bucket page.  On the delete objects page, you will see the name of the file and a confirmation to type 'delete' and clicked on 'delete objects'

Now when you go back to the s3 bucket and enable 'Show versions' you will see another version of the file with type as 'Delete Marker'but size 0. The Delete Marker makes AWS S3 behave as if the object has been deleted.

    *   Delete Marker object does not have any data or ACL associated with it, just the key and the version ID
    *   An object retrieval on a bucket with a delete marker as the Current version would return a 404
    *   Only a DELETE operation is allowed on the Delete Marker object
    *   If the Delete marker object is deleted by specifying its version ID, the previous non-current version object 
        becomes current version object
    *   If a DELETE request is fired on an object with Delete Marker as the current version, the Delete marker object 
        is not deleted but a Delete Marker is added again

        If you have 'Show Versions' enabled and you select all the version of the file you wish to delete, then the files will be permanently 
        deleted

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q09: Explain S3 Encryption for objects?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    Their are primarily 4 methods for object encryption in S3
        a.  SSE-S3  :   Encrypts S3 objects, using keys handled and managed by AWS on the ServerSide
        b.  SSE-KMS :   Leverage AWS Key Management Service to manage encryption keys on the ServerSide
        c.  SSE-C   :   When the customer wants to manage the encryption keys on the ServerSide
        d.  Client Side Encryption

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q10: Explain SSE-S3 encryption?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
SSE-S3: Its a server side encryption scheme, where the object encryption happens, using keys handled and managed by S3 with AES-256 algorithm. To upload an object and set the SSE as AES256, you must set the header as 'x-amz-server-side-encryption':'AES256'

![!](../Assets/S3/SSE-S3-Encryption.PNG)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q11: Explain SSE-KMS encryption?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
SSE-KMS: Its a server side encryption scheme, where the object encryption happens, using keys handled and managed by KMS.KMS provides advantages in terms of User Control and Audit Trail. To upload an object and set the SSE as AES256, you must set the header as 'x-amz-server-side-encryption':'aws:kms'

![!](../Assets/S3/SSE-KMS-Encryption.PNG)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q12: Explain SSE-C encryption?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
SSE-C: Its a server side encryption scheme, where the object encryption happens using data keys fully managed by customer outside AWS. AWS-S3, DOES NOT store the encryption keys that you provide. HTTPS is a must for this type of encryption, and is ONLY available using AWS CLI

Special Note: For both uploading and downloading the data, the client side data keys must be passed in to the header

![!](../Assets/S3/SSE-C-Encryption.PNG)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q13: Explain Client Side Encryption?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
For client side encryption, the object is encrypted before its being uploaded to the S3 bucket. In this encryption scheme, clients MUST encrypt the object themselves before sending to S3, and also MUST decrypt data themselves when retrieving data from S3.
A client library, such as Amazon S3 encryption client can be used for encryption / decryption. The customer manages the keys and the encryption cycle.

![!](../Assets/S3/SSE-ClientSide-Encryption.PNG)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q14: Apart from the object encryption schemes, how else can we encrypt the objects?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Rather than selecting the encryption scheme for each object while its being uploaded, we can also set a 'DEFAULT' encryption scheme at the BUCKET Level, so that every time an object is being uploaded, it will have the default encryption scheme attached to it.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q15: Explain AWS - S3 Security?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    AWS S3 has following types of security
        a.  User Based:     IAM Policies: Identify which api calls should be allowed for a specific user from IAM console.

        b.  Resource Based:
            1.  Bucket Policies:    Bucket wide rules from the S3 console, which allows cross account access.
            2.  Object Access Control List (ACL):   Rarely used.
            3.  Bucket Access Control List (ACL):   Rarely used.

        c.  Network Based:
            1.  Supports VPC endpoints (for instances in VPC without internet)

        Special Note:
            An IAM Principal (user or group) can access an S3 object if:
            1.  The user IAM permissions allow it OR the resource policy allows it
            2.  AND their is NO explicit deny
        
        S3 Logging and Auditing:
            1.  S3 Access Logs can be stored in other S3 bucket
            2.  API Calls can be logged in AWS Cloud Trail

        User Security:
            1.  MFA Delete: MFA can be enabled in versioned buckets to delete objects
            2.  Pre-Signed URLs: URLs that are valid for a limited time can be managed using pre-signed url (temporary authentication)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q16: Explain S3 Bucket Policy?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    S3 Bucket Policy, is JSON based policy, here's an example

            {
                "Version": "2023-02-13",
                "Statement": [
                    {
                        "Sid": "PublicRead",
                        "Effect": "Allow",
                        "Principal": "*",
                        "Action": ["s3: GetObject"],
                        "Resource": ["arn:aws:s3:::examplebucket/*"]
                    }
                ]
            }

    Here:
        Resource: Are buckets and objects
        Actions: Set of API to Allow or deny
        Effect: Allow or deny
        Principal: The account or user to apply the policy to

    You can use S3 Bucket policy to:
        1.  Grant public Access to the bucket
        2.  Force objects to be encrypted at upload
        3.  Grant Access to another account (Cross Account)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q17: Explain Bucket settings for Block Public Access?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    Block public access to buckets and objects granted through:
        a.  New Access Control List (ACLs)
        b.  Any Access Control List (ACLs)
        c.  New Public Bucket or Access Point Policies

        Block public and cross-account access to buckets and objects through any public bucket or access point policies.
        These were created to prevent company data leaks

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q18: Explain S3 Websites?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
S3 can host static websites and have them accesible on the world wide web. The website URLs will look like the following.
<bucket-name>.s3-website.<AWS-region>.amazonaws.com
If you get a 403 (forbidden) error, make sure, the bucket policy allows public reads.

-----------------------------------------------------------------------------------------------------------------------------------------------------------    
Q19: How to create a S3 website?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
S3 website can be use to host static webpages. Along with the webpages, you will need two files, once is index.html and other is error.html, which will look like the following.

    1.  index.html

            <html>
                <head>
                    <titile>My First Webpage</title>
                </head>
                <body>
                    <h1>I love coffe</h1>
                    <p>Hellow World!</p>
                </body>

                <img src="coffee.jpg" width=500/>
            </html>
        
    2.  error.html
        
        <h1>Uh oh, there was an error</h1>

    Following are the steps:
        a.  On the S3 dashboard, click on 'Buckets'
        b.  Then click on the Bucket Name that is already created.
        c.  Click on 'Properties' of the bucket.
        d.  Scroll to the bottom of the page, and then click on 'Edit' for 'Static Website Hosting'
        e.  'Enable' the static website option, select 'Hosting Type' as 'Host a Static Website'
        f.  Upload the 'index.html' and provide the name of the 'Index Document' as 'index.html'
        g.  Upload the 'error.html' and provide the name of the 'Error Document' as 'error.html'
        h.  Click on 'Permissions' of the bucket.
        i.  Edit the 'Block Public Access' and turn it off.
        j.  Edit the 'Bucket Policy', then click on 'Policy Generator'
        k.  On the new webpage, there are following options
            1.  Select Policy Type:- as 'S3 Bucket Policy'
                SQS Queue Policy, S3 Bucket Policy, VPC Endpoint Policy, IAM Policy, SNS Topic Policy

            2.  Under Add Statement(s)
                Effect: Allow
                Principal: *    (So that any body can access the static pages)
                AWS Service: Will be automatically selected as 'Amazon S3' based on #1
                Actions: GetObject
                Amazon Resource Name (ARN): Will be available on the 'Edit Bucket Policy page'
        l.  Now, if you try to access the static website, using the URL under 'Static Website Hosting', you should be able to see 
            the page.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q20: What is CORS or Cross-Origin-Resource-Sharing?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    An Origin is a scheme (protocol), host(domain) and port
        e.g.    https://www.example.com: [Implied: port is 443 for HTTPS and 80 for HTTP]

        CORS is : Web-Browser based mechanism to allow requests to other origins while visiting the main origin, given that they have 
            access to other origins.
        
        Here are the examples of
        a.  Same origins        : http://example.com/app1 and http://example.com/app2
        b.  Different Origins   : http://www.example.com and http://other.example.com

        The requests wont be fullfilled unless the other origin allows for the requests, using CORS headers 
        (e.g.: Access-Control-Allow-Origin)

        Here's the flow:
        Consider their are two servers (host1: https://www.example.com; host2: https://www.other.example.com), a web-browser makes a 
        request to host1, and host1 tell the web-browser that you need to talk to host2 to get the actual files (since they are hosted
         there). In turn, the web-browser then talks to host2 sending something called as a 'Pre-Flight Request' which looks like 
         the following

        Pre-flight Request
        ------------------------------------------------
        | OPTIONS/                                      |
        | Host: www.other.com                           |
        | Origin: https://www.example.com               |
        ------------------------------------------------

        Host2 (which is Cross Origin in this case) provides a pre-flight response
        ---------------------------------------------------------
        |Access-Control-Allow-Origin: https://www.example.com   |
        |Access-Control-Allow-Methods:GET, PUT, DELETE          |
        ---------------------------------------------------------

        Then the web-browser, requests an allowed method (for e.g. GET to Host2) and Host2 responds back with the resource
        
        Another Example:
        We have two S3 buckets, with Static Website Hosting enabled, the first S3 bucket, only have the index.html and the error.html
        While the 2nd S3 bucket, has the assets. In this case, the web-browser will talk to the first S3 bucket, since it has the 
        index.html.and then it will talk to the 2nd S3 bucket which has the assets that can be returned back.

        In this case, the 2nd S3 bucket is the Cross-Origin, and if the web-browser is to get the requested resource, the 'Cross-Origin' 
        must be enabled at the second S3 bucket.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q21: Explain S3 - Consistency Model?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    S3 has strong consitency as of december 2020
        a.  After a:
            1.  Successful write of a new object (new PUT)
            2.  Or an overwrite or delete of an existing object (otherwise PUT or DELETE)
        
        b.  After any:
            1.  Subsequent read request immediately receives the latest version of the object (read after write consistency)
            2.  Subsequent list request immediately reflects changes (list consistency)
        No additional charges for this strong consistency and no performance impact.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q22: What are S3 Access Logs
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    If you need to log the access requests coming into S3, you can use S3 access logs. This is for audit purpose. The results of the 
    access requestsmay be 'authorized' / 'denied' and all of it will be logged into another S3 bucket.

    Once the data is inside the S3 bucket, it can analyzed using any data analysis tool or in Amazon Athena

    The log format is at:
    https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html


    ----------
    |{USER-1}|---|
    ----------   | Requests
    |{USER-2}|---|<-------> {S3 Bucket}
    ----------   |              |
    |{USER-3}|---|              | Log All Requests
    ----------                  | 
                                V
                            {Logging Bucket}
        
    Logging Bucket should always be a different bucket, so maintain the size of the bucket. Functionally its allowed to have the same 
    bucket to be the application bucket and the logging bucket, but depending on the traffic and size of the buket, if we add logging 
    data (creating logging loops) to the same bucket, the size will grow exponentially.
    
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q23: How to enable S3 Access Logs:
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    Once you have created a new bucket which will be the 'Logging-Bucket' for the 'Monitored-Bucket'.
    On the S3 dashboard, click on the Left Panel Menu - 'Buckets'
        a.  Click on the Bucket (Monitored Bucket) you wish to log the access requests
        b.  Click on 'Properties' and then click on 'Server access logging'. Its 'Disabled' by default, click on edit and enable it.
        c.  Once you enable it, the page will ask for 'Target Bucket'
        d.  Browse the 'Bucket-Name' where you want the access logs to be written.
        e.  Be VERY CAREFUL, not to select the same bucket (as the monitored bucket, else you'll end up with a huge AWS bill)
    
        Special Note: Once you enable the server access logging, the 'Bucket Policy' gets updated to be able to 'write' logs to another 
        bucket (S3 Log Delivery group)
        
        Bucket Policy: When you enable server access logging, the S3 console automatically updates your bucket policy to include access 
        to the S3 log delivery group.

        It takes about an hour or two to have the Access logs written to the target bucket

        Its a good idea to add an additional directory inside the target S3 bucket to be able to store logs from multiple buckets 
        logically
        
        e.g. Target Bucket: s3://samarth-ashish-aws-developer-021223/s3-021623-bucket-logs/

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q24: Explain S3 Replication?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    AWS S3 has two types of replication
        a.  CRR :   Cross Region Replication
        b.  SRR :   Same Region Replication

        For both of these replication types, Bucket versioning MUST be enabled in both Source Bucket and Target Bucket
        The source bucket and target bucket can be in different accounts.
        Must have appropriate permissions (for source bucket to copy items over to target bucket)


        ----------------        Asynchronous Replication        ----------------
        [ Source Bucket]--------------------------------------> [Target Bucket]
        ----------------                                        ----------------

        If both the buckets are in the same region: Use Same Region Replication
        If the buckets are NOT in the same region: Use Cross Region Replication

        USE CASES:
        CRR :   Compliance, Disastor Recovery, Lower Latency Access, Replication Cross account
        SRR :   Log Aggregation, Live Replication between production and test accounts.

        Special Note: Once the replication is activated, Only NEW objects or NEW versions of existing object will be replicated 
            (old objects will not be replicated).
            However, you can manually copy the previous objects
        
        Objects with DELETE marker are not replicated by default, but can be enabled.
        Their is no 'Chaining of Replication'
            : If bucket 1 has replication into bucket 2, which has replication into bucket 3. then objects created in bucket 1 
            are not replicated to bucket 3

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q25: How to enable S3 replication?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    For S3 replication to be enabled, the pre-requisite it to have 'Versioning' Enabled on both the source bucket and the target bucket.
        Once the pre-requisite is completed / verified, follow these steps:
        a.  Click on the source bucket and navigate to 'Management' 
        b.  Under 'Replication Rules' click on 'Create Replication Rule' 
        c.  On the 'Create Replication Rule' page: provide the following
            1.  A replication rule name of your choice.
            2.  Source Bucket:
                i.  Chose a rule scope:
                    *.  Limit the scope of this rule using one or more filters  : Use this if you want to filter objects by their type
                    *.  Apply to all objects in the bucket
                ii. Destination:
                    *.  Choose a bucket in this account
                    *.  Specify a bucket in another account     : This one is easier
                iii.IAM Role
                    *.  Select 'Choose from existing IAM rules' and then select 'Create New Role' form the drop down.
            3.  Click Save and your replication rules are set.

        Note: 
            Replication rules will work only on new objects or new version of same objects that are uploaded to source S3 bucket.
            Objects with 'Delete Markers' are not replicated by default.
    
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q26: What are S3-Pre-signed URLs?
----------------------------------------------------------------------------------------------------------------------------------------------------------- 
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    s3: presign :   The presign command generates a pre-signed URL for a key in the S3 bucket. You can use this command to generate URLs that can be used by 
            others to access a file in the specified S3 bucket key.
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    SCN#01  :   Generate a pre-signed url
    Command :   aws s3 presign s3://bucket_name/samplePrefix/file_name.png --region us-east-1 --expires-in 3600
        
    /*
    Sample Output:
    Output:
    https://s3.ap-south-1.amazonaws.com/bucket_name/samplePrefix/file_name.png?
    X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA4MCZT73PAX7ZMVFW%2F20220314%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20220314T054113Z&X-Amz-Expires=3600
    &X-Amz-SignedHeaders=host&X-Amz-Signature=f14608bbf3e1f9f8d215eb5b439b87e167b1055bcd7a45c13a33debd3db1be96
    */

    S3 pre-signed urls can be generated using CLI or SDK.
        For Downloads, it can be easily generated using the CLI
        For Uploads, it can be easily generated using the SDK
        
    Valid for a default of 3600 seconds, but is configurable to change the timeout with '--expires-in[time_by_seconds]' argument

    Important Note: Users with a pre-signed URL inherit the permissions of the persion who generated the URL for allowed actions 
    (GET / PUT)

    Examples:
        a.  Allow only logged in users to download a premium video on your S3 bucket
        b.  Allow an ever changing list of users to download files by generating urls dynamically
        c.  Allow temporarily a user to upload to a precise location in our bucket.
        
    Note: If you get any issues you can run the following command to configure
            aws configure set default.s3.signature_version s3v4
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q27: Explain S3-Storage Classes?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    Following are the S3-Storage Classes
        a.  AWS S3 Standard - General purpose
            1.  High Durability (99.999999999999%) (11- 9s) of objects across multiple AZ
            2.  If you stire 10,000,000 objects with AWS S3, on an average you can expect to incur loss of a single object once 
                every 10000 years
            3.  99.99% Availability over a give year
            4.  Can sustain 2 Concurrent Facility Failures
            5.  Use Cases:
                i.  Big Data Analytics, Mobile & Gaming Applications, Content Distribution

        b.  AWS S3 Standard-Infrequent Access (IA)
            1.  Suitable for data that is less frequently accessed, but requires rapid access when needed
            2.  High Durability (99.999999999999%) (10- 9s) of objects across multiple AZ
            3.  99.99% Availability over a give year
            4.  Low cost compared to AWS S3 standard, and minimum storage duration is 30 days
            5.  Can sustain 2 Concurrent Facility Failures
            6.  Use Cases:
                i.  As a Data Store for Disastor Recovery, Backups

        c.  AWS S3 One Zone-Infrequent Access
            1.  Same as Infrequent Access, but data is stored in a single AZ
            2.  High Durability (99.999999999999%) (11- 9s) of objects across SINGLE AZ
            3.  99.5 % Availability over a give year
            4.  Low latency and high throughput performance
            5.  Supports SSL for data at transit and encryption at rest
            6.  Low cost compared to IA (by 20%) and minimum storage duration is 30 days
            7.  Use Cases:
                i.  Storing Secondary Backup Copies of on-premise data, or storing data that you can recreate.

        d.  AWS S3 Intelligent Tiering
            1.  Low latency and high throughput performance as S3 standard
            2.  Small monthly monitoring and auto-tiering fee
            3.  Automatically moves objects between two access tiers (S3 Standard and S3-IA) based on changing access patterns
            4.  High Durability (99.999999999999%) (11- 9s) of objects across multiple AZ
            5.  Resilient against events that impact and entire AZ
            6.  99.99% Availability over a give year
            7.  Minimum storage duration is 30 days

        e.  AWS Glacier
            1.  Low cost object storage meant for archiving / Backup
            2.  Data is retained for the longer term (10+ years)
            3.  Alternative to on-premise magnetic tape storage
            4.  Average durability is 99.99999999999 % (11 - 9s)
            5.  Cost per storage per month ($ 0.004 / GB) + retrieval cost
            6.  Each Item in Glacier is called 'ARCHIVE' (instead of Objects in S3) and each archive be up to 40 TB
            7.  Archives are stored in 'Vaults'

            Retrieval Options
            a.  Expedited (1 to 5 minutes) for files less than 250 MB
            b.  Standard (3 t0 5 hours)
            c.  Bulk (5 to 12 hours)

            Minimum Storage duration is 90 days

        f.  AWS Glacier Deep Archive
            1. For even longer term storage and cheaper than AWS Glacier
            Retrieval Options
            a.  Standard (12 hours)
            b.  Bulk (48 hours)

            Minimum Storage duration is 180 days

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q28: Explain S3-Lifecycle Rules?
-----------------------------------------------------------------------------------------------------------------------------------------------------------    
S3 Lifecycle rules defines, 'What to do with the objects in S3' in an automated way, there are two types of access supported by S3 - Lifecycle

    a.  Transition Actions: It defines when objects are transitioned to another storage class
        1.  Move objects (current or previos if versioning is enabled) to Standard IA class 60 days after creation
        2.  Move to Glacier for archiving after 6 months
        
    b.  Expiration Actions: Conigure objects to expire (Delete) after some time
        1.  Access log files can be set to delete after 365 days
        2.  Can be used to delete old version of files (if versioning is enabled)
        3.  Can be used to delete incomplete multi-part Uploads
        
    Rules can be created for a certain prefix (e.g. : s3://my_bucket/ics-files/*)
    Rules can be created for a certain object tags (e.g. : Department: Finance)

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q29: Explain S3 - KMS Limitation?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
If you use SSE-KMS, you may be impacted by KMS Limits

Here's the explanation: When you upload an object, S3 calls the 'GenerateDataKey' KMS API, when you download it, it call the Decrypt API. These calls gets counted against the KMS quota per second (it can be 5500 or 10000 or 300000 requests per second based on the region). This may eventually limit the performance if your S3 objects are in the high read / write application.

-----------------------------------------------------------------------------------------------------------------------------------------------------------    
Q30: How to enhance S3 performance?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    These are the following ways to enhance S3 performance
        a.  Multi-Part Upload: This is a mechanism to 'parallelize' upload of a file by breaking it to small (multiple pieces). 
            This method is recommended for files > 100 MB, and are a MUST for files > 5GB. 'Parallelization' helps to speed up the 
            transfer, and S3 is smart enough to rejoin these small parts and create the same file again.

        b.  S3 Transfer Acceleration: Suppose you want to make an object available in Australia, but you are in USA. So this 
            mechanism allows you to upload the object using edge locations in USA (via public internet) and then AWS can upload 
            the same object to S3 bucket in Australia using AWS private network. This process is called, S3-Transfer Acceleration

        c.  S3 Byte-Range Fetches: While the above two mechanism are good for upload, the following is better for downloads. When you 
            want to download a big file from S3 bucket, AWS runs something called as Parallelize GETs by requesting specific byte range. 
            
            For e.g. if you install a docker image, which may be big, its starts 
            to download the image in multiple parts say for e.g. 10 files with each having a size of 10 mb each and it also shows 
            the progress bar).
            
            These Parallelize GETs behave the same way and asks for multiple parts with each part for a specific range
            (0-10, 11-20, 21-30 etc). 
            
            This mechanism is very resilient to failure and save a lot of time and bandwidth. This is also very useful, 
            if you only want the file / object header before the actual file, so in that case you can only request the 
            header information (which may be first 50 bytes of the file), rather than downloading the entire file.

-----------------------------------------------------------------------------------------------------------------------------------------------------------    
Q31: Explain S3-Select / Glacier-Select?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Anyone who works with data has probably used a cloud-based storage solution at some point in their data pipeline. For example, if you need the first 1000 lines of a 1 million line dataset you have to retrieve the entire dataset and filter locally. This makes it impractical to use Amazon S3 as the sole database for applications, particularly apps that constantly need to retrieve subsets of a larger dataset.
         
Amazon S3 Select lets you use simple SQL expressions to pull out only the data you need from an Amazon S3 object With S3 Select you no longer need to retrieve an entire dataset only to retain a few lines. Instead, you can use classic SQL expressions to query data in place and retrieve only a subset, such as data between two dates or above a certain price. Currently Amazon S3 Select works with objects in CSV, JSON, and Apache Parquet format By reducing the volume of data that has to be loaded and processed by your applications, S3 Select can improve the performance of most applications that frequently access data from S3 by up to 400%

    The best part of Amazon S3 Select is how it simplifies application structure. Prior to Amazon S3 Select, 
    the typical structure of a web app may have looked like the following

        a.  Store data in a classic SQL (i.e. MySQL or PostgreSQL) or NoSQL (i.e. DynamoDB) database where it can be queried
        b.  Keep backup copies of the data in persistent object storage (i.e. Amazon S3)
        c.  Transfer new data into Amazon S3 and then add to the database.

        With Amazon S3 Select the structure of the web app looks like the following
        a.  Store data in Amazon S3 where it can be queried.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q32: Explain S3 Object Lock and Glacier Vault Lock?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    S3 Object Lock:
        Follows the WORM model: Write Once, Read Many
        Block an object version, no one can delete or modify

    Glacier Object Lock: 
        Follows the WORM model: Write Once, Read Many
        Block an object version, no one can delete or modify

    Note:
        1.  Lock can be applied at both, Object Level & Bucket Level
        2.  Lock has the following modes
            a.  Governance Mode: Users with special permissions can override the rules and retention period
            b.  Compliance Mode: No One, even the root user cannot overide the rules and retention period.

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Q33: Explain S3 Event Notifications?
-----------------------------------------------------------------------------------------------------------------------------------------------------------
    If some events happen in your S3 bucket, like the following
        a.  A new object has been created       :   S3:ObjectCreated
        b.  An existing object has been removed :   S3:ObjectRemoved
        c.  A deleted object has been restored  :   S3:ObjectRestore
        d.  A replication event has started     :   S3:Replication


    And you want to be able to react to these events, so you can create event notifications rules. 
    Following notifications services can be used
        1.  SNS: Simple Notifications Service
        2.  SQS: Simple Queue Service
        3.  Lambda Function

    Note: S3 Event Notifications typically deliver events in seconds but can sometimes take a minute of two.
        1.  If two writes are made to a single non-versioned object at the same time, its possible that 
            only a single event notification will be sent
        2.  If you want to ensure that an event notification is sent for every successful write, you can 
            enable versioning on your bucket.

-----------------------------------------------------------------------------------------------------------------------------------------------------------