-----------------------------------------------------------------------------------------------------------------------------------------------------------
AWS - S3 : SimpleStorageService
-----------------------------------------------------------------------------------------------------------------------------------------------------------

AWS S3: It advertised as the infinetly scaling storage, hence we do not define its size while provisioning it (unlink EBS or EFS). It allows the users to 
    store objects (files) in 'buckets' (directories).
    a.  Buckets MUST have a globally-unique-name
    b.  Buckets are defined at the regional level
    c.  Follows a specific naming convention
        1.  No Uppercase
        2.  NO underscore
        3.  Between 3-63 characters long
        4.  Not an IP
        5.  Must Start with lowercase letter or number
    d.  OBJECTS (files) have a key, the key is basically the complete path to the file, for.e.g.
        1.  s3://my-bucket/my_file.txt                              :   here the 'key' is : my_file.txt
        2.  s3://my-bucket/root-folder/child-folder/my_file.txt     :   here the 'key' is : root-folder/child-folder/my_file.txt
    e.  OBJECT (files) have a value as well. The values of these keys is basically, the content of the file
        1.  Maximum Object size in S3 is 5TB
        2.  If uploading more than 5GB in one go, the user must use 'Multi-Part' upload.
        3.  If uploading a file more than 160 GB, you must use AWS-CLI, AWS SDK, or AWS S3 Rest API
        4.  Each object in S3, can have 'metadata' (filename, fileowner, created-date, some system data etc)
        5.  Each object in S3, can have Tags (a maximum of 10, which is useful for security / identification and lifecycle management)
        6.  Each object in S3, can have its Version ID (if versioning is enabled)

    Q01: What is the minimum policy required to be able to create a S3 Bucket?
    A01: The minimum role required for a user to be able to create an S3 bucket is - AmazonS3FullAccess

    Q02: How to create a S3 Bucket?
    A02: Following are the steps to create a S3 bucket
        a.  Login to the AWS management console with a user that has the permissions to create a S3 Bucket
        b.  Search for S3 on the AWS management console
        c.  On the S3 dashboard, click on 'Create Bucket'
        d.  Provide a 'Bucket Name' to the bucket [This name must be globally unique]
        e.  Select the AWS region where you want to create the bucket [Even though S3 is global service, but buckets are regional]
        f.  On the 'Object Ownership section, select 'ACLs Disabled' [All accounts in this bucket are owned by this account, Access to the bucket 
            and its objects is specified using only policies]
        g.  'Block all public access' is enabled by default.
        h.  On 'Bucket Versioning', 'disabled' is selected by default
        i.  Default Encryption is selected as - 'Amazon-S3-managed-keys'
        j.  Under 'Advanced Settings' - Object Lock is 'disabled' by default
            [Store objects using a write-once-read-many (WORM) model to help you prevent objects from being deleted or overwritten for a fixed 
            amount of time or indefinitely]
        k.  Once the bucket is created, you can click on the bucket name and check the properties / permissions / Metrics / Access points etc.
    
    Q03: How to upload an object in to an S3 bucket?
    A03: Once the S3 bucket is created:
        a.  Click on the 'Buckets' section of S3 Dashboard.
        b.  Click on the bucket name, and then click on upload.
        c.  Under 'Upload' click on 'Add Files' or 'Add Folders' and navigate to the file or folder on your machine.
        d.  'Destination URL' will be prebuilt based on your S3 bucket.
        e.  Select the 'Storage Class' and then click on 'Upload'
        f.  Once the object is uploaded successfully, it will visible under the 'Bucket' --> 'Objects'

    Q04: How many storage classes are there in S3?
    A04: S3 has the following storage classes.
        a.  Standard                :   Designed for                    : Frequently Accessed Data (more than once a month) with milliseconds access
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : None
                                        Minimum Billable Objext size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : None
        
        b.  Intelligent Teiring     :   Designed for                    : Data with changing and unknown access patterns
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : None
                                        Minimum Billable Objext size    : None
                                        Monitoring & Auto tiering fees  : Per-object fees apply for objects >= 128kb
                                        Retrieval fees                  : None

        c.  Standard-IA             :   Designed for                    : Infrequently accessed data (once a month) with milliseconds access
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 30 Days
                                        Minimum Billable Objext size    : 128 KB
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        d.  One Zone-IA             :   Designed for                    : Recreatable, infrequently accessed data (once a month) stored in a single 
                                                                          Availability Zone with milliseconds access
                                        Available in                    : 1 AZ
                                        Minimum Storage Duration        : 30 Days
                                        Minimum Billable Objext size    : 128 KB
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        e.  Glacier Instant Retrieval:  Designed for                    : Long-lived archive data accessed once a quarter with instant retrieval in 
                                                                          milliseconds
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 90 Days
                                        Minimum Billable Objext size    : 128 KB
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        f.  Glacier Flexible Retrieval: Designed for                    : Long-lived archive data accessed once a year with retrieval of minutes to hours
            (formerly Glacier)
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 90 Days
                                        Minimum Billable Objext size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        g.  Glacier Deep Archive    :   Designed for                    : Long-lived archive data accessed less than once a year with retrieval of hours
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 180 Days
                                        Minimum Billable Objext size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply

        h.  Reduced Redundancy      :   Designed for                    : Noncritical, frequently accessed data with milliseconds access 
                                                                          (not recommended as S3 Standard is more cost effective)
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : None
                                        Minimum Billable Objext size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply

    Q05: How to check the properties of an uploaded object?
    A05: Following are the steps:
        a.  Once you are inside the S3 bucket, click on the object, you wish to check the properties for.
        b.  A new standard dialog will open and the 'Properties' tab will be pre-selected, their are two more tabs, Permissions and Versions
        c.  On this page, their are 4 more options on the right top [Copy S3 URI, DOWNLOAD, OPEN, Object Options]
        d.  If you wish to see the object content without downloading it, simply click on 'OPEN'
        e.  When you click on 'Open', AWS opens a link which points to this object and has a pre-signed URL with AWS security token
        f.  If you try to open the same objec using 'Object URL', it will fail, since this URL does not have the pre-signed AWS security token
    
    Q06: Explain key points of S3 Bucket versioning feature?
    A06: Amazon S3 allows for versioning of the files
        a.  Its enabled at the bucket level
        b.  Same Key overwrite will increment the 'Version': 1,2,3 etc.
        c.  Its a best practice to version your buckets.
            1.  So that you are protected against unintended deletes (ability to restore a version)
            2.  Easy Rollback to a previous version
        
        Special Note:
            1.  Any file, that is not versioned prior to enabling versioning, will have version id as NULL
            2.  Suspending versioning, does not delete the previous versions.
    
    Q07: What happens if you upload a file with the same name with S3 Bucket versioning enabled?
    A07: If we upload two files of the same name with S3 bucket versioning enabled, then both the file will be enabled and will have 'Version ID'
        populated with some 'string value' as the version id of both the files.
        The version of the file can be seen, under the 'Bucket' - 'Show Versions', and both the files with the same name but different version ids
        will be shown to the user.
    
    Q08: What is the difference between 'delete' vs 'Permanently Delete' in S3 Bucket?
    A08: Suppose you attempt to delete a file, (which has multiple versions, but since 'Show Versions' is disabled, you cant see those versions).
        using the 'checkbox' and 'delete' button on the s3 bucket page.
        On the delete objects page, you will see the name of the file and a confirmation to type 'delete' and clicked on 'delete objects'

        Now when you go back to the s3 bucket and enable 'Show versions' you will see another version of the file with type as 'Delete Marker'
        but size 0.The Delete Marker makes AWS S3 behave as if the object has been deleted.

        *   Delete Marker object does not have any data or ACL associated with it, just the key and the version ID
        *   An object retrieval on a bucket with a delete marker as the Current version would return a 404
        *   Only a DELETE operation is allowed on the Delete Marker object
        *   If the Delete marker object is deleted by specifying its version ID, the previous non-current version object becomes current version object
        *   If a DELETE request is fired on an object with Delete Marker as the current version, the Delete marker object is not deleted but a 
            Delete Marker is added again

        If you have 'Show Versions' enabled and you select all the version of the file you wish to delete, then the files will be permanently deleted

    Q09: Explain S3 Encryption for objects?
    A09: Their are primarily 4 methods for object encryption in S3
        a.  SSE-S3  :   Encrypts S3 objects, using keys handled and managed by AWS on the ServerSide
        b.  SSE-KMS :   Leverage AWS Key Management Service to manage encryption keys on the ServerSide
        c.  SSE-C   :   When the customer wants to manage the encryption keys on the ServerSide
        d.  Client Side Encryption

    Q10: Explain SSE-S3 encryption?
    A10: SSE-S3: Its a server side encryption scheme, where the object encryption happens, using keys handled and managed by S3 with AES-256 algorithm.
        To upload an object and set the SSE as AES256, you must set the header as 'x-amz-server-side-encryption':'AES256'
        Here's the flow:


                                                                    |-----------------------------------------------------|
                         HTTP/S + HEADER                            |   |~~~~~~~~~~~~~~~~~~~~~|                     S3    |
        __________       ('x-amz-server-side-encryption':'AES256')  |   | [ object ]          | Encryption                |
        | OBJECT |--------------------------------------------------->  |    +                |-----------> { BUCKET}     |
        ----------                                                  |   | S3 Managed Data Key |                           |
                                                                    |   ~~~~~~~~~~~~~~~~~~~~~~                            | 
                                                                    |-----------------------------------------------------|

    Q11: Explain SSE-KMS encryption?
    A11: SSE-KMS: Its a server side encryption scheme, where the object encryption happens, using keys handled and managed by KMS.
        KMS provides advantages in terms of User Control and Audit Trail.
        To upload an object and set the SSE as AES256, you must set the header as 'x-amz-server-side-encryption':'aws:kms'
        Here's the flow:

                                                                    |-----------------------------------------------------------|
                         HTTP/S + HEADER                            |   |~~~~~~~~~~~~~~~~~~~~~~~~~~~|                     S3    |
        __________       ('x-amz-server-side-encryption':'AES256')  |   | [ object ]                | Encryption                |
        | OBJECT |--------------------------------------------------->  |    +                      |-----------> { BUCKET}     |
        ----------                                                  |   | KMS Customer Master Key   |                           |
                                                                    |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                           | 
                                                                    |-----------------------------------------------------------|
    
    Q12: Explain SSE-C encryption?
    A12: SSE-C: Its a server side encryption scheme, where the object encryption happens using data keys fully managed by customer outside AWS.
        AWS-S3, DOES NOT store the encryption keys that you provide.
        HTTPS is a must for this type of encryption, and is ONLY availabel using AWS CLI
        Here's the flow:

    ----------------------------------------                                      |-----------------------------------------------------------|  
    |    _____________                      |                                     |                                                     S3    |
    |    [   OBJECT  ]                      |                                     | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                             |
    |    -------------                      |   HTTPS ONLY + DATA KEY in HEADER   | | [ Object ]                | Encryption                  |
    |          +                            |------------------------------------>| |   +                       |--------------> { BUCKET }   |
    |    _____________________________      |                                     | | [Client Provided Data Key]|                             |
    |    [   CLIENT SIDE DATA KEYS   ]      |                                     | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                             |
    |    -----------------------------      |                                     |                                                           |
    |----------------------------------------                                     |-----------------------------------------------------------|  

    Special Note: For both uploading and downloading the data, the client side data keys must be passed in to the header

    Q13: Explain Client Side Encryption?
    A13: For client side encryption, the object is encryption before its being uploaded to the S3 bucket. In this encryption scheme, clients 
        MUST encrypt the object themselves before sending to S3, and also MUST decrypt data themselves when retrieving data from S3.
        A client library, such as Amazon S3 encryption client can be used to encryption / decryption. The customer manages the keys and the 
        encryption cycle.
        Here's the file:

    |----------------------------------------------------------------|
    |                   Client - S3 Encryption SDK                   |
    |  ~~~~~~~~~~~~~~~~~~~~~~~~~~                                    |              |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    |  | [   OBJECT  ]          | Encryption                         |  HTTPS       |                       S3       |
    |  |         +              |----------------> [Encrypted Obkect]|------------> |       {   BUCKET  }            |
    |  | [Client Side Data Key] |                                    |              |                                |
    |  ~~~~~~~~~~~~~~~~~~~~~~~~~~                                    |              |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    |                                                                |
    |----------------------------------------------------------------|    

    Q14: Apart from the object encryption schemes, how else can we encrypt the objects?
    A14: Rather than selecting the encryption scheme for each object while its being upload, we can also set a 'DEFAULT' encryption scheme 
        at the BUCKET Level, so that every time an object is being uploaded, it will have the default encryption scheme attached to it.
    
    Q15: Explain AWS - S3 Security?
    A15: AWS S3 has two types of security
        a.  User Based:     IAM Policies: Identify which api calls should be allowed for a specific user from IAM console.
        b.  Resource Based:
            1.  Bucket Policies:    Bucket wide rules from the S3 console, which allows cross account access.
            2.  Object Access Control List (ACL):   Rarely used.
            3.  Bucket Access Control List (ACL):   Rarely used.
        c.  Network Based:
            1.  Suports VPC endpoints (for instances in VPC without internet)

        Special Note:
            An IAM Principal (user or group) can access an S3 object if:
            1.  The user IAM permissions allow it OR the resource policy allows it
            2.  AND their is NO explicit deny
        
        S3 Logging and Auditing:
            1.  S3 Access Logs can be stored in other S3 bucket
            2.  API Calls can be logged in AWS Cloud Trail

        User Security:
            1.  MFA Delete: MFA can be enabled in versioned buckets to delete objects
            2.  Pre-Signed URLs: URLs that are valid for a limited time can be manaegd using pre-signed url (temporary authentication)

    Q16: Explain S3 Bucket Policy?
    A16: S3 Bucket Policy, is JSON based policy, here's an example
            {
                "Version": "2023-02-13",
                "Statement": [
                    {
                        "Sid": "PublicRead",
                        "Effect": "Allow",
                        "Principal": "*",
                        "Action": ["s3: GetObject"],
                        "Resource": ["arn:aws:s3:::examplebucket/*"]
                    }
                ]
            }

        Here:
        Resource: Are buckets and objects
        Actions: Set of API to Allow or deny
        Effect: Allow or deny
        Principal: The account or user to apply the policy to

        You can use S3 Bucket policy to:
        1.  Grant public Access to the bucket
        2.  Force objects to be encrypted at upload
        3.  Grant Access to another account (Cross Account)
    
    Q14: Explain Bucket settings for Block Public Access?
    A14: Block public access to buckets and objects granted through:
        a.  New Access Control List (ACLs)
        b.  Any Access Control List (ACLs)
        c.  New Public Bucket or Access Point Policies

        Block public and cross-account access to buckets and objects through any public bucket or access point policies.
        These were created to prevent company data leaks

    Q15: Explain S3 Websites?
    A15: S3 can host static websites and have them accesible on the world wide web. The website URLs will look like the following.
        <bucket-name>.s3-website.<AWS-region>.amazonaws.com
        If you get a 403 (forbidden) error, make sure, the bucket policy allows public reads.
    
    Q16: How to create a S3 website?
    A16: S3 website can be use to host static webpages. Along with the webpages, you will need two files, once is index.html and other is
        error.html, which will look like the following.
        1.  index.html

            <html>
                <head>
                    <titile>My First Webpage</title>
                </head>
                <body>
                    <h1>I love coffe</h1>
                    <p>Hellow World!</p>
                </body>

                <img src="coffee.jpg" width=500/>
            </html>
        
        2.  error.html
        
            <h1>Uh oh, there was an error</h1>

        Following are the steps:
        a.  On the S3 dashboard, click on 'Buckets'
        b.  Then click on the Bucket Name that is already created.
        c.  Click on 'Properties' of the bucket.
        d.  Scroll to the bottom of the page, and then click on 'Edit' for 'Static Website Hosting'
        e.  'Enable' the static website option, select 'Hosting Type' as 'Host a Static Website'
        f.  Upload the 'index.html' and provide the name of the 'Index Document' as 'index.html'
        g.  Upload the 'error.html' and provide the name of the 'Error Document' as 'error.html'
        h.  Click on 'Permissions' of the bucket.
        i.  Edit the 'Block Public Access' and turn it off.
        j.  Edit the 'Bucket Policy', then click on 'Policy Generator'
        k.  On the new webpage, there are following options
            1.  Select Policy Type:- as 'S3 Bucket Policy'
                SQS Queue Policy, S3 Bucket Policy, VPC Endpoint Policy, IAM Policy, SNS Topic Policy

            2.  Under Add Statement(s)
                Effect: Allow
                Principal: *    (So that any body can access the static pages)
                AWS Service: Will be automatically selected as 'Amazon S3' based on #1
                Actions: GetObject
                Amazon Resource Name (ARN): Will be available on the 'Edit Bucket Policy page'
        l.  Now, if you try to access the static website, using the URL under 'Static Website Hosting', you should be able to see the page.

    Q17: What is CORS or Cross-Origin-Resource-Sharing?
    A17: An Origin is a scheme (protocol), host(domain) and port
        e.g.    https://www.example.com: [Implied: port is 443 for HTTPS and 80 for HTTP]

        CORS is : Web-Browser based mechanism to allow requests to other origins while visiting the main origin, given that they have 
            access to other origins.
        
        Here are the examples of
        a.  Same origins        : http://example.com/app1 and http://example.com/app2
        b.  Different Origins   : http://www.example.com and http://other.example.com

        The requests wont be fullfilled unless the other origin allows for the requests, using CORS headers (e.g.: Access-Control-Allow-Origin)

        Here's the flow:
        Consider their are two servers (host1: https://www.example.com; host2: https://www.other.example.com), a web-browser makes a request to 
        host1, and host1 tell the web-browser that you need to talk to host2 to get the actual files (since they are hosted there). In turn, the 
        web-browser then talks to host2 sending something called as a 'Pre-Flight Request' which looks like the following

        Pre-flight Request
        ------------------------------------------------
        | OPTIONS/                                      |
        | Host: www.other.com                           |
        | Origin: https://www.example.com               |
        ------------------------------------------------

        Host2 (which is Cross Origin in this case) provides a pre-flight response
        ---------------------------------------------------------
        |Access-Control-Allow-Origin: https://www.example.com   |
        |Access-Control-Allow-Methods:GET, PUT, DELETE          |
        ---------------------------------------------------------

        Then the web-browser, requests an allowed method (for e.g. GET to Host2) and Host2 responds back with the resource
        