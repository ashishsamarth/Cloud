-----------------------------------------------------------------------------------------------------------------------------------------------------------
AWS - S3 : SimpleStorageService
-----------------------------------------------------------------------------------------------------------------------------------------------------------

AWS S3: It advertised as the infinitely scaling storage, hence we do not define its size while provisioning it (unlink EBS or EFS). It allows the users to 
    store objects (files) in 'buckets' (directories).
    a.  Buckets MUST have a globally-unique-name
    b.  Buckets are defined at the regional level
    c.  Follows a specific naming convention
        1.  No Uppercase
        2.  NO underscore
        3.  Between 3-63 characters long
        4.  Not an IP
        5.  Must Start with lowercase letter or number
    d.  OBJECTS (files) have a key, the key is basically the complete path to the file, for.e.g.
        1.  s3://my-bucket/my_file.txt                              :   here the 'key' is : my_file.txt
        2.  s3://my-bucket/root-folder/child-folder/my_file.txt     :   here the 'key' is : root-folder/child-folder/my_file.txt
    e.  OBJECT (files) have a value as well. The values of these keys is basically, the content of the file
        1.  Maximum Object size in S3 is 5TB
        2.  If uploading more than 5GB in one go, the user must use 'Multi-Part' upload.
        3.  If uploading a file more than 160 GB, you must use AWS-CLI, AWS SDK, or AWS S3 Rest API
        4.  Each object in S3, can have 'metadata' (filename, fileowner, created-date, some system data etc)
        5.  Each object in S3, can have Tags (a maximum of 10, which is useful for security / identification and lifecycle management)
        6.  Each object in S3, can have its Version ID (if versioning is enabled)

    Q01: What is the minimum policy required to be able to create a S3 Bucket?
    A01: The minimum role required for a user to be able to create an S3 bucket is - AmazonS3FullAccess

    Q02: How to create a S3 Bucket?
    A02: Following are the steps to create a S3 bucket
        a.  Login to the AWS management console with a user that has the permissions to create a S3 Bucket
        b.  Search for S3 on the AWS management console
        c.  On the S3 dashboard, click on 'Create Bucket'
        d.  Provide a 'Bucket Name' to the bucket [This name must be globally unique]
        e.  Select the AWS region where you want to create the bucket [Even though S3 is global service, but buckets are regional]
        f.  On the 'Object Ownership section, select 'ACLs Disabled' [All accounts in this bucket are owned by this account, Access to the bucket 
            and its objects is specified using only policies]
        g.  'Block all public access' is enabled by default.
        h.  On 'Bucket Versioning', 'disabled' is selected by default
        i.  Default Encryption is selected as - 'Amazon-S3-managed-keys'
        j.  Under 'Advanced Settings' - Object Lock is 'disabled' by default
            [Store objects using a write-once-read-many (WORM) model to help you prevent objects from being deleted or overwritten for a fixed 
            amount of time or indefinitely]
        k.  Once the bucket is created, you can click on the bucket name and check the properties / permissions / Metrics / Access points etc.
    
    Q03: How to upload an object in to an S3 bucket?
    A03: Once the S3 bucket is created:
        a.  Click on the 'Buckets' section of S3 Dashboard.
        b.  Click on the bucket name, and then click on upload.
        c.  Under 'Upload' click on 'Add Files' or 'Add Folders' and navigate to the file or folder on your machine.
        d.  'Destination URL' will be prebuilt based on your S3 bucket.
        e.  Select the 'Storage Class' and then click on 'Upload'
        f.  Once the object is uploaded successfully, it will visible under the 'Bucket' --> 'Objects'

    Q04: How many storage classes are there in S3?
    A04: S3 has the following storage classes.
        a.  Standard                :   Designed for                    : Frequently Accessed Data (more than once a month) with milliseconds access
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : None
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : None
        
        b.  Intelligent Tiering     :   Designed for                    : Data with changing and unknown access patterns
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : None
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : Per-object fees apply for objects >= 128kb
                                        Retrieval fees                  : None

        c.  Standard-IA             :   Designed for                    : Infrequently accessed data (once a month) with milliseconds access
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 30 Days
                                        Minimum Billable Object size    : 128 KB
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        d.  One Zone-IA             :   Designed for                    : Re-creatable, infrequently accessed data (once a month) stored in a single 
                                                                          Availability Zone with milliseconds access
                                        Available in                    : 1 AZ
                                        Minimum Storage Duration        : 30 Days
                                        Minimum Billable Object size    : 128 KB
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        e.  Glacier Instant Retrieval:  Designed for                    : Long-lived archive data accessed once a quarter with instant retrieval in 
                                                                          milliseconds
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 90 Days
                                        Minimum Billable Object size    : 128 KB
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        f.  Glacier Flexible Retrieval: Designed for                    : Long-lived archive data accessed once a year with retrieval of minutes to hours
            (formerly Glacier)
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 90 Days
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply
        
        g.  Glacier Deep Archive    :   Designed for                    : Long-lived archive data accessed less than once a year with retrieval of hours
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : 180 Days
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply

        h.  Reduced Redundancy      :   Designed for                    : Noncritical, frequently accessed data with milliseconds access 
                                                                          (not recommended as S3 Standard is more cost effective)
                                        Available in                    : More than or equal to 3 AZs
                                        Minimum Storage Duration        : None
                                        Minimum Billable Object size    : None
                                        Monitoring & Auto tiering fees  : None
                                        Retrieval fees                  : Per-GB fees apply

    Q05: How to check the properties of an uploaded object?
    A05: Following are the steps:
        a.  Once you are inside the S3 bucket, click on the object, you wish to check the properties for.
        b.  A new standard dialog will open and the 'Properties' tab will be pre-selected, their are two more tabs, Permissions and Versions
        c.  On this page, their are 4 more options on the right top [Copy S3 URI, DOWNLOAD, OPEN, Object Options]
        d.  If you wish to see the object content without downloading it, simply click on 'OPEN'
        e.  When you click on 'Open', AWS opens a link which points to this object and has a pre-signed URL with AWS security token
        f.  If you try to open the same object using 'Object URL', it will fail, since this URL does not have the pre-signed AWS security token
    
    Q06: Explain key points of S3 Bucket versioning feature?
    A06: Amazon S3 allows for versioning of the files
        a.  Its enabled at the bucket level
        b.  Same Key overwrite will increment the 'Version': 1,2,3 etc.
        c.  Its a best practice to version your buckets.
            1.  So that you are protected against unintended deletes (ability to restore a version)
            2.  Easy Rollback to a previous version
        
        Special Note:
            1.  Any file, that is not versioned prior to enabling versioning, will have version id as NULL
            2.  Suspending versioning, does not delete the previous versions.
    
    Q07: What happens if you upload a file with the same name with S3 Bucket versioning enabled?
    A07: If we upload two files of the same name with S3 bucket versioning enabled, then both the file will be enabled and will have 'Version ID'
        populated with some 'string value' as the version id of both the files.
        The version of the file can be seen, under the 'Bucket' - 'Show Versions', and both the files with the same name but different version ids
        will be shown to the user.
    
    Q08: What is the difference between 'delete' vs 'Permanently Delete' in S3 Bucket?
    A08: Suppose you attempt to delete a file, (which has multiple versions, but since 'Show Versions' is disabled, you cannot see those versions).
        using the 'checkbox' and 'delete' button on the s3 bucket page.
        On the delete objects page, you will see the name of the file and a confirmation to type 'delete' and clicked on 'delete objects'

        Now when you go back to the s3 bucket and enable 'Show versions' you will see another version of the file with type as 'Delete Marker'
        but size 0. The Delete Marker makes AWS S3 behave as if the object has been deleted.

        *   Delete Marker object does not have any data or ACL associated with it, just the key and the version ID
        *   An object retrieval on a bucket with a delete marker as the Current version would return a 404
        *   Only a DELETE operation is allowed on the Delete Marker object
        *   If the Delete marker object is deleted by specifying its version ID, the previous non-current version object becomes current version object
        *   If a DELETE request is fired on an object with Delete Marker as the current version, the Delete marker object is not deleted but a 
            Delete Marker is added again

        If you have 'Show Versions' enabled and you select all the version of the file you wish to delete, then the files will be permanently deleted

    Q09: Explain S3 Encryption for objects?
    A09: Their are primarily 4 methods for object encryption in S3
        a.  SSE-S3  :   Encrypts S3 objects, using keys handled and managed by AWS on the ServerSide
        b.  SSE-KMS :   Leverage AWS Key Management Service to manage encryption keys on the ServerSide
        c.  SSE-C   :   When the customer wants to manage the encryption keys on the ServerSide
        d.  Client Side Encryption

    Q10: Explain SSE-S3 encryption?
    A10: SSE-S3: Its a server side encryption scheme, where the object encryption happens, using keys handled and managed by S3 with AES-256 algorithm.
        To upload an object and set the SSE as AES256, you must set the header as 'x-amz-server-side-encryption':'AES256'
        Here's the flow:


                                                                    |-----------------------------------------------------|
                         HTTP/S + HEADER                            |   |~~~~~~~~~~~~~~~~~~~~~|                     S3    |
        __________       ('x-amz-server-side-encryption':'AES256')  |   | [ object ]          | Encryption                |
        | OBJECT |--------------------------------------------------->  |    +                |-----------> { BUCKET}     |
        ----------                                                  |   | S3 Managed Data Key |                           |
                                                                    |   ~~~~~~~~~~~~~~~~~~~~~~                            | 
                                                                    |-----------------------------------------------------|

    Q11: Explain SSE-KMS encryption?
    A11: SSE-KMS: Its a server side encryption scheme, where the object encryption happens, using keys handled and managed by KMS.
        KMS provides advantages in terms of User Control and Audit Trail.
        To upload an object and set the SSE as AES256, you must set the header as 'x-amz-server-side-encryption':'aws:kms'
        Here's the flow:

                                                                    |-----------------------------------------------------------|
                         HTTP/S + HEADER                            |   |~~~~~~~~~~~~~~~~~~~~~~~~~~~|                     S3    |
        __________       ('x-amz-server-side-encryption':'AES256')  |   | [ object ]                | Encryption                |
        | OBJECT |--------------------------------------------------->  |    +                      |-----------> { BUCKET}     |
        ----------                                                  |   | KMS Customer Master Key   |                           |
                                                                    |   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                           | 
                                                                    |-----------------------------------------------------------|
    
    Q12: Explain SSE-C encryption?
    A12: SSE-C: Its a server side encryption scheme, where the object encryption happens using data keys fully managed by customer outside AWS.
        AWS-S3, DOES NOT store the encryption keys that you provide.
        HTTPS is a must for this type of encryption, and is ONLY available using AWS CLI
        Here's the flow:

    ----------------------------------------                                      |-----------------------------------------------------------|  
    |    _____________                      |                                     |                                                     S3    |
    |    [   OBJECT  ]                      |                                     | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                             |
    |    -------------                      |   HTTPS ONLY + DATA KEY in HEADER   | | [ Object ]                | Encryption                  |
    |          +                            |------------------------------------>| |   +                       |--------------> { BUCKET }   |
    |    _____________________________      |                                     | | [Client Provided Data Key]|                             |
    |    [   CLIENT SIDE DATA KEYS   ]      |                                     | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                             |
    |    -----------------------------      |                                     |                                                           |
    |----------------------------------------                                     |-----------------------------------------------------------|  

    Special Note: For both uploading and downloading the data, the client side data keys must be passed in to the header

    Q13: Explain Client Side Encryption?
    A13: For client side encryption, the object is encryption before its being uploaded to the S3 bucket. In this encryption scheme, clients 
        MUST encrypt the object themselves before sending to S3, and also MUST decrypt data themselves when retrieving data from S3.
        A client library, such as Amazon S3 encryption client can be used to encryption / decryption. The customer manages the keys and the 
        encryption cycle.
        Here's the file:

    |----------------------------------------------------------------|
    |                   Client - S3 Encryption SDK                   |
    |  ~~~~~~~~~~~~~~~~~~~~~~~~~~                                    |              |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    |  | [   OBJECT  ]          | Encryption                         |  HTTPS       |                       S3       |
    |  |         +              |----------------> [Encrypted Object]|------------> |       {   BUCKET  }            |
    |  | [Client Side Data Key] |                                    |              |                                |
    |  ~~~~~~~~~~~~~~~~~~~~~~~~~~                                    |              |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    |                                                                |
    |----------------------------------------------------------------|    

    Q14: Apart from the object encryption schemes, how else can we encrypt the objects?
    A14: Rather than selecting the encryption scheme for each object while its being upload, we can also set a 'DEFAULT' encryption scheme 
        at the BUCKET Level, so that every time an object is being uploaded, it will have the default encryption scheme attached to it.
    
    Q15: Explain AWS - S3 Security?
    A15: AWS S3 has two types of security
        a.  User Based:     IAM Policies: Identify which api calls should be allowed for a specific user from IAM console.
        b.  Resource Based:
            1.  Bucket Policies:    Bucket wide rules from the S3 console, which allows cross account access.
            2.  Object Access Control List (ACL):   Rarely used.
            3.  Bucket Access Control List (ACL):   Rarely used.
        c.  Network Based:
            1.  Supports VPC endpoints (for instances in VPC without internet)

        Special Note:
            An IAM Principal (user or group) can access an S3 object if:
            1.  The user IAM permissions allow it OR the resource policy allows it
            2.  AND their is NO explicit deny
        
        S3 Logging and Auditing:
            1.  S3 Access Logs can be stored in other S3 bucket
            2.  API Calls can be logged in AWS Cloud Trail

        User Security:
            1.  MFA Delete: MFA can be enabled in versioned buckets to delete objects
            2.  Pre-Signed URLs: URLs that are valid for a limited time can be managed using pre-signed url (temporary authentication)

    Q16: Explain S3 Bucket Policy?
    A16: S3 Bucket Policy, is JSON based policy, here's an example
            {
                "Version": "2023-02-13",
                "Statement": [
                    {
                        "Sid": "PublicRead",
                        "Effect": "Allow",
                        "Principal": "*",
                        "Action": ["s3: GetObject"],
                        "Resource": ["arn:aws:s3:::examplebucket/*"]
                    }
                ]
            }

        Here:
        Resource: Are buckets and objects
        Actions: Set of API to Allow or deny
        Effect: Allow or deny
        Principal: The account or user to apply the policy to

        You can use S3 Bucket policy to:
        1.  Grant public Access to the bucket
        2.  Force objects to be encrypted at upload
        3.  Grant Access to another account (Cross Account)
    
    Q14: Explain Bucket settings for Block Public Access?
    A14: Block public access to buckets and objects granted through:
        a.  New Access Control List (ACLs)
        b.  Any Access Control List (ACLs)
        c.  New Public Bucket or Access Point Policies

        Block public and cross-account access to buckets and objects through any public bucket or access point policies.
        These were created to prevent company data leaks

    Q15: Explain S3 Websites?
    A15: S3 can host static websites and have them accesible on the world wide web. The website URLs will look like the following.
        <bucket-name>.s3-website.<AWS-region>.amazonaws.com
        If you get a 403 (forbidden) error, make sure, the bucket policy allows public reads.
    
    Q16: How to create a S3 website?
    A16: S3 website can be use to host static webpages. Along with the webpages, you will need two files, once is index.html and other is
        error.html, which will look like the following.
        1.  index.html

            <html>
                <head>
                    <titile>My First Webpage</title>
                </head>
                <body>
                    <h1>I love coffe</h1>
                    <p>Hellow World!</p>
                </body>

                <img src="coffee.jpg" width=500/>
            </html>
        
        2.  error.html
        
            <h1>Uh oh, there was an error</h1>

        Following are the steps:
        a.  On the S3 dashboard, click on 'Buckets'
        b.  Then click on the Bucket Name that is already created.
        c.  Click on 'Properties' of the bucket.
        d.  Scroll to the bottom of the page, and then click on 'Edit' for 'Static Website Hosting'
        e.  'Enable' the static website option, select 'Hosting Type' as 'Host a Static Website'
        f.  Upload the 'index.html' and provide the name of the 'Index Document' as 'index.html'
        g.  Upload the 'error.html' and provide the name of the 'Error Document' as 'error.html'
        h.  Click on 'Permissions' of the bucket.
        i.  Edit the 'Block Public Access' and turn it off.
        j.  Edit the 'Bucket Policy', then click on 'Policy Generator'
        k.  On the new webpage, there are following options
            1.  Select Policy Type:- as 'S3 Bucket Policy'
                SQS Queue Policy, S3 Bucket Policy, VPC Endpoint Policy, IAM Policy, SNS Topic Policy

            2.  Under Add Statement(s)
                Effect: Allow
                Principal: *    (So that any body can access the static pages)
                AWS Service: Will be automatically selected as 'Amazon S3' based on #1
                Actions: GetObject
                Amazon Resource Name (ARN): Will be available on the 'Edit Bucket Policy page'
        l.  Now, if you try to access the static website, using the URL under 'Static Website Hosting', you should be able to see the page.

    Q17: What is CORS or Cross-Origin-Resource-Sharing?
    A17: An Origin is a scheme (protocol), host(domain) and port
        e.g.    https://www.example.com: [Implied: port is 443 for HTTPS and 80 for HTTP]

        CORS is : Web-Browser based mechanism to allow requests to other origins while visiting the main origin, given that they have 
            access to other origins.
        
        Here are the examples of
        a.  Same origins        : http://example.com/app1 and http://example.com/app2
        b.  Different Origins   : http://www.example.com and http://other.example.com

        The requests wont be fullfilled unless the other origin allows for the requests, using CORS headers (e.g.: Access-Control-Allow-Origin)

        Here's the flow:
        Consider their are two servers (host1: https://www.example.com; host2: https://www.other.example.com), a web-browser makes a request to 
        host1, and host1 tell the web-browser that you need to talk to host2 to get the actual files (since they are hosted there). In turn, the 
        web-browser then talks to host2 sending something called as a 'Pre-Flight Request' which looks like the following

        Pre-flight Request
        ------------------------------------------------
        | OPTIONS/                                      |
        | Host: www.other.com                           |
        | Origin: https://www.example.com               |
        ------------------------------------------------

        Host2 (which is Cross Origin in this case) provides a pre-flight response
        ---------------------------------------------------------
        |Access-Control-Allow-Origin: https://www.example.com   |
        |Access-Control-Allow-Methods:GET, PUT, DELETE          |
        ---------------------------------------------------------

        Then the web-browser, requests an allowed method (for e.g. GET to Host2) and Host2 responds back with the resource
        
        Another Example:
        We have two S3 buckets, which Static Website Hosting enabled, the first S3 bucket, only have the index.html and the error.html
        While the 2nd S3 bucket, has the assets. In this case, the web-browser will talk to the first S3 bucket, since it has the index.html.
        and then it will talk to the 2nd S3 bucket which has the assets that can be returned back.
        In this case, the 2nd S3 bucket is the Cross-Origin, and if the web-browser is to get the requested resource, the 'Cross-Origin' 
        must be enabled at the second S3 bucket.
    
    Q18: Explain S3 - Consistency Model?
    A18: S3 has strong consitency as of december 2020
        a.  After a:
            1.  Successful write of a new object (new PUT)
            2.  Or an overwrite or delete of an existing object (otherwise PUT or DELETE)
        
        b.  After any:
            1.  Subsequent read request immediately receives the latest version of the object (read after write consistency)
            2.  Subsequent list request immediately reflects changes (list consistency)
        No additional charges for this strong consistency and no performance impact.

    Q19: What are S3 Access Logs
    A19: If you need to log the access requests coming into S3, you can use S3 access logs. This is for audit purpose. The results of the access requests
        may be 'authorized' / 'denied' and all of it will be logged into another S3 bucket.

        Once the data is inside the S3 bucket, it can analyzed using any data analysis tool or in Amazon Athena

        The log format is at:
        https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html


        ----------
        |{USER-1}|---|
        ----------   | Requests
        |{USER-2}|---|<-------> {S3 Bucket}
        ----------   |              |
        |{USER-3}|---|              | Log All Requests
        ----------                  | 
                                    V
                                {Logging Bucket}
        
        Logging Bucket should always be a different bucket, so maintain the size of the bucket. Functionally its allowed to have the same bucket to be the 
        application bucket and the logging bucket, but depending on the traffic and size of the buket, if we add logging data (creating logging loops) to 
        the same bucket, the size will grow exponentially.
    
    Q20: How to enable S3 Access Logs:
    A20: Once you have created a new bucket which will be the 'Logging-Bucket' for the 'Monitored-Bucket'.
        On the S3 dashboard, click on the Left Panel Menu - 'Buckets'
        a.  Click on the Bucket (Monitored Bucket) you wish to log the access requests
        b.  Click on 'Properties' and then click on 'Server access logging'. Its 'Disabled' by default, click on edit and enable it.
        c.  Once you enable it, the page will ask for 'Target Bucket'
        d.  Browse the 'Bucket-Name' where you want the access logs to be written.
        e.  Be VERY CAREFUL, not to select the same bucket (as the monitored bucket, else you'll end up with a huge AWS bill)
    
        Special Note: Once you enable the server access logging, the 'Bucket Policy' gets updated to be able to 'write' logs to another bucket (S3 Log Delivery group)
        Bucket Policy: When you enable server access logging, the S3 console automatically updates your bucket policy to include access to the S3 log delivery group.

        It takes about an hour or two to have the Access logs written to the target bucket

        Its a good idea to add an additional directory inside the target S3 bucket to be able to store logs from multiple buckets logically
        e.g. Target Bucket: s3://samarth-ashish-aws-developer-021223/s3-021623-bucket-logs/


    Q21: Explain S3 Replication?
    A21: AWS S3 has two types of replication
        a.  CRR :   Cross Region Replication
        b.  SRR :   Same Region Replication

        For both of these replication types, Bucket versioning MUST be enabled in both Source Bucket and Target Bucket
        The source bucket and target bucket can be in different accounts.
        Must have appropriate permissions (for source bucket to copy items over to target bucket)


        ----------------        Asynchronous Replication        ----------------
        [ Source Bucket]--------------------------------------> [Target Bucket]
        ----------------                                        ----------------

        If both the buckets are in the same region: Use Same Region Replication
        If the buckets are NOT in the same region: Use Cross Region Replication

        USE CASES:
        CRR :   Compliance, Disastor Recovery, Lower Latency Access, Replication Cross account
        SRR :   Log Aggregation, Live Replication between production and test accounts.

        Special Note: Once the replication is activated, Only NEW objects or NEW versions of existing object will be replicated (old objects will not be replicated).
            However, you can manually copy the previous objects
        
        Objects with DELETE marker are not replicated by default, but can be enabled.
        Their is no 'Chaining of Replication'
            : If bucket 1 has replication into bucket 2, which has replication ino bucket 3. then objects created in bucket 1 are not replicated to bucket 3

    Q22: How to enable S3 replication?
    A22: For S3 replication to be enabled, the pre-requisite it to have 'Versioning' Enabled on both the source bucket and the target bucket.
        Once the pre-requisite is completed / verified, follow these steps:
        a.  Click on the source bucket and navigate to 'Management' 
        b.  Under 'Replication Rules' click on 'Create Replication Rule' 
        c.  On the 'Create Replication Rule' page: provide the following
            1.  A replication rule name of your choice.
            2.  Source Bucket:
                i.  Chose a rule scope:
                    *.  Limit the scope of this rule using one or more filters  : Use this if you want to filter objects by their type
                    *.  Apply to all objects in the bucket
                ii. Destination:
                    *.  Choose a bucket in this account
                    *.  Specify a bucket in another account     : This one is easier
                iii.IAM Role
                    *.  Select 'Choose from existing IAM rules' and then select 'Create New Role' form the drop down.
            3.  Click Save and your replication rules are set.

        Note: 
            Replication rules will work only on new objects or new version of same objects that are uploaded to source S3 bucket.
            Objects with 'Delete Markers' are not replicated by default.
        
    Q23: What are S3-Pre-signed URLs?
    A23: 
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        s3: presign :   The presign command generates a pre-signed URL for a key in the S3 bucket. You can use this command to generate URLs that can be used by 
                others to access a file in the specified S3 bucket key.
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        SCN#01  :   Generate a pre-signed url
        Command :   aws s3 presign s3://bucket_name/samplePrefix/file_name.png --region us-east-1 --expires-in 3600
        
        /*
        Sample Output:
        Output:
        https://s3.ap-south-1.amazonaws.com/bucket_name/samplePrefix/file_name.png?
        X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA4MCZT73PAX7ZMVFW%2F20220314%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20220314T054113Z&X-Amz-Expires=3600
        &X-Amz-SignedHeaders=host&X-Amz-Signature=f14608bbf3e1f9f8d215eb5b439b87e167b1055bcd7a45c13a33debd3db1be96
        */

        S3 pre-signed urls ca be generated using CLI or SDK.
            For Downloads, it can be easily generated using the CLI
            For Uploads, it can be easily generated using the SDK
        
        Valid for a default of 3600 seconds, but is configurable to change the timeout with '--expires-in[time_by_seconds]' argument

        Important Note: Users with a pre-signed URL inherit the permissions of the persion who generated the URL for allowed actions (GET / PUT)

        Examples:
            a.  Allow only logged in users to download a premium video on your S3 bucket
            b.  Allow an ever changing list of users to download files by generating urls dynamically
            c.  Allow temporarily a user to upload to a precise location in our bucket.
        
        Note: If you get any issues you can run the following command to configure
            aws configure set default.s3.signature_version s3v4
    
    Q24: Explain S3-Storage Classes?
    A24: Following are the S3-Storage Classes
        a.  AWS S3 Standard - General purpose
            1.  High Durability (99.999999999999%) (11- 9s) of objects across multiple AZ
            2.  If you stire 10,000,000 objects with AWS S3, on an average you can expect to incur loss of a single object once 
                every 10000 years
            3.  99.99% Availability over a give year
            4.  Can sustain 2 Concurrent Facility Failures
            5.  Use Cases:
                i.  Big Data Analytics, Mobile & Gaming Applications, Content Distribution

        b.  AWS S3 Standard-Infrequent Access (IA)
            1.  Suitable for data that is less frequently accessed, but requires rapid access when needed
            2.  High Durability (99.999999999999%) (10- 9s) of objects across multiple AZ
            3.  99.99% Availability over a give year
            4.  Low cost compared to AWS S3 standard, and minimum storage duration is 30 days
            5.  Can sustain 2 Concurrent Facility Failures
            6.  Use Cases:
                i.  As a Data Store for Disastor Recovery, Backups

        c.  AWS S3 One Zone-Infrequent Access
            1.  Same as Infrequent Access, but data is stored in a single AZ
            2.  High Durability (99.999999999999%) (11- 9s) of objects across SINGLE AZ
            3.  99.5 % Availability over a give year
            4.  Low latency and high throughput performance
            5.  Supports SSL for data at transit and encryption at rest
            6.  Low cost compared to IA (by 20%) and minimum storage duration is 30 days
            7.  Use Cases:
                i.  Storing Secondary Backup Copies of on-premise data, or storing data that you can recreate.

        d.  AWS S3 Intelligent Tiering
            1.  Low latency and high throughput performance as S3 standard
            2.  Small monthly monitoring and auto-tiering fee
            3.  Automatically moves objects between two access tiers (S3 Standard and S3-IA) based on changing access patterns
            4.  High Durability (99.999999999999%) (11- 9s) of objects across multiple AZ
            5.  Resilient against events that impact and entire AZ
            6.  99.99% Availability over a give year
            7.  Minimum storage duration is 30 days

        e.  AWS Glacier
            1.  Low cost object storage meant for archiving / Backup
            2.  Data is retained for the longer term (10+ years)
            3.  Alternative to on-premise magnetic tape storage
            4.  Average durability is 99.99999999999 % (11 - 9s)
            5.  Cost per storage per month ($ 0.004 / GB) + retrieval cost
            6.  Each Item in Glacier is called 'ARCHIVE' (instead of Objects in S3) and each archive be up to 40 TB
            7.  Archives are stored in 'Vaults'

            Retrieval Options
            a.  Expedited (1 to 5 minutes) for files less than 250 MB
            b.  Standard (3 t0 5 hours)
            c.  Bulk (5 to 12 hours)

            Minimum Storage duration is 90 days

        f.  AWS Glacier Deep Archive
            1. For even longer term storage and cheaper than AWS Glacier
            Retrieval Options
            a.  Standard (12 hours)
            b.  Bulk (48 hours)

            Minimum Storage duration is 180 days
